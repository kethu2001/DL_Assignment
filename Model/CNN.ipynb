{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0eba36-0342-436e-b804-6a40c332604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: split_folders in d:\\anaconda\\lib\\site-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install split_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8883c17c-616b-4c18-9fef-340dc682e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70615c93-fcd7-4086-9347-c705cec5db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 15515 files [00:33, 461.24 files/s]\n"
     ]
    }
   ],
   "source": [
    "input_folder = r\"C:\\Users\\Ishan's MSi\\Desktop\\Dataset\\garbage_classification\"\n",
    "output = r\"C:\\Users\\Ishan's MSi\\Desktop\\Dataset\\process_data\"\n",
    "splitfolders.ratio(input_folder, output, seed=42, ratio=(.6, .2, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d1d797-21ee-4c07-9db5-4bbdf3406129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ratio in module splitfolders.split:\n",
      "\n",
      "ratio(input, output='output', seed=1337, ratio=(0.8, 0.1, 0.1), group_prefix=None, move=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(splitfolders.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a55cb4-fbf9-410f-b45b-db2dfb7494c6",
   "metadata": {},
   "source": [
    "#Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2c363e-f926-46b5-ad50-6c33c95b19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b07d09-3468-4a02-98c7-ef299ffd3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn_model(input_shape=(224, 224, 3), num_classes=12):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional Layer 1\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Flatten the layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layer\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a982b1c1-d315-46d2-9ca5-42a197e5c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "def save_model(model, filepath='garbage_classification_model.h5'):\n",
    "    model.save(filepath)\n",
    "    print(f\"Model saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d069c34-815e-4e72-8e81-af33e42a2a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9307 images belonging to 12 classes.\n",
      "Found 3100 images belonging to 12 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 982ms/step - accuracy: 0.3937 - loss: 1.8798 - val_accuracy: 0.5658 - val_loss: 1.3239\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/290\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 292ms/step - accuracy: 0.4688 - loss: 1.3816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4688 - loss: 1.3816 - val_accuracy: 0.5714 - val_loss: 1.2797\n",
      "Epoch 3/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 560ms/step - accuracy: 0.5476 - loss: 1.3589 - val_accuracy: 0.6527 - val_loss: 1.0818\n",
      "Epoch 4/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - accuracy: 0.5000 - loss: 1.2726 - val_accuracy: 0.7143 - val_loss: 0.9512\n",
      "Epoch 5/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 568ms/step - accuracy: 0.6121 - loss: 1.1897 - val_accuracy: 0.6735 - val_loss: 1.0339\n",
      "Epoch 6/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - accuracy: 0.5000 - loss: 1.7334 - val_accuracy: 0.5714 - val_loss: 1.1283\n",
      "Epoch 7/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 572ms/step - accuracy: 0.6444 - loss: 1.0930 - val_accuracy: 0.6956 - val_loss: 0.9339\n",
      "Epoch 8/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.6562 - loss: 0.9776 - val_accuracy: 0.6786 - val_loss: 1.2721\n",
      "Epoch 9/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 566ms/step - accuracy: 0.6744 - loss: 1.0171 - val_accuracy: 0.7129 - val_loss: 0.8876\n",
      "Epoch 10/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - accuracy: 0.5312 - loss: 1.3257 - val_accuracy: 0.6071 - val_loss: 1.0685\n",
      "Epoch 11/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 562ms/step - accuracy: 0.6925 - loss: 0.9608 - val_accuracy: 0.7184 - val_loss: 0.8930\n",
      "Epoch 12/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - accuracy: 0.7812 - loss: 0.8555 - val_accuracy: 0.6429 - val_loss: 1.1353\n",
      "Epoch 13/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 565ms/step - accuracy: 0.6970 - loss: 0.9234 - val_accuracy: 0.7370 - val_loss: 0.8573\n",
      "Epoch 14/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - accuracy: 0.8125 - loss: 0.6392 - val_accuracy: 0.7500 - val_loss: 0.6257\n",
      "Epoch 15/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 588ms/step - accuracy: 0.7148 - loss: 0.8760 - val_accuracy: 0.7253 - val_loss: 0.8669\n",
      "Epoch 16/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - accuracy: 0.6562 - loss: 1.2250 - val_accuracy: 0.8571 - val_loss: 0.5264\n",
      "Epoch 17/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 671ms/step - accuracy: 0.7173 - loss: 0.8479 - val_accuracy: 0.7344 - val_loss: 0.8307\n",
      "Epoch 18/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777us/step - accuracy: 0.6250 - loss: 0.9026 - val_accuracy: 0.7143 - val_loss: 0.8423\n",
      "Epoch 19/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 717ms/step - accuracy: 0.7329 - loss: 0.8018 - val_accuracy: 0.7493 - val_loss: 0.8258\n",
      "Epoch 20/20\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.6875 - loss: 0.6932 - val_accuracy: 0.8214 - val_loss: 0.5482\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 174ms/step - accuracy: 0.7724 - loss: 0.7536\n",
      "Validation accuracy: 75.42%\n",
      "Found 3108 images belonging to 12 classes.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 485ms/step\n",
      "Confusion Matrix\n",
      "[[ 14  14   5   5  73   9   3  13  14  24   8   7]\n",
      " [  8  13   4   9  77   7   5  20  11  30   6   7]\n",
      " [  7  11   0   8  40   5   2   9   9  17   8   6]\n",
      " [ 10   7   5   6  63   4   4   8  12  37  11  12]\n",
      " [ 88  78  25  48 370  48  19  85  57 157  52  38]\n",
      " [ 15  12   3   6  35  10   5   8   4  18   7   4]\n",
      " [  7  11   7   8  65   6  11  12   4  17   5   2]\n",
      " [  8  12   6  10  79  12   6  17   9  32  13   6]\n",
      " [ 21  10   6   8  51  13   2  18  11  20   6   7]\n",
      " [ 32  23  13  11 144  21   4  26  18  72  15  17]\n",
      " [ 12  13   4   4  51   8   3   7   4  21   5   8]\n",
      " [  8   5   3   8  70   9   6  15   8  15   5   3]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.06      0.07      0.07       189\n",
      "  biological       0.06      0.07      0.06       197\n",
      " brown-glass       0.00      0.00      0.00       122\n",
      "   cardboard       0.05      0.03      0.04       179\n",
      "     clothes       0.33      0.35      0.34      1065\n",
      " green-glass       0.07      0.08      0.07       127\n",
      "       metal       0.16      0.07      0.10       155\n",
      "       paper       0.07      0.08      0.08       210\n",
      "     plastic       0.07      0.06      0.07       173\n",
      "       shoes       0.16      0.18      0.17       396\n",
      "       trash       0.04      0.04      0.04       140\n",
      " white-glass       0.03      0.02      0.02       155\n",
      "\n",
      "    accuracy                           0.17      3108\n",
      "   macro avg       0.09      0.09      0.09      3108\n",
      "weighted avg       0.17      0.17      0.17      3108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define image size, batch size, epochs, and learning rate\n",
    "image_size = (150, 150)\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the directories for the dataset (train, validation, test)\n",
    "train_dir = r\"C:\\Users\\Ishan's MSi\\Desktop\\Dataset\\process_data\\train\"\n",
    "val_dir = r\"C:\\Users\\Ishan's MSi\\Desktop\\Dataset\\process_data\\val\"\n",
    "test_dir = r\"C:\\Users\\Ishan's MSi\\Desktop\\Dataset\\process_data\\test\"\n",
    "\n",
    "\n",
    "# Pre-processing and data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Creating data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional and Pooling layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers and adding fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Helps to prevent overfitting\n",
    "model.add(Dense(12, activation='softmax'))  # 12 classes for classification\n",
    "\n",
    "# Compiling the model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=val_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f'Validation accuracy: {val_acc * 100:.2f}%')\n",
    "\n",
    "# Test on test data\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = Y_pred.argmax(axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "\n",
    "print('Classification Report')\n",
    "target_names = list(train_generator.class_indices.keys())\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c850c0ef-17a6-4da8-8c7b-1dacc8d5fc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07055f3-8d79-426a-8c64-7199e738ecba",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'D:\\CNN.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCNN.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Prediction and Confusion Matrix\u001b[39;00m\n\u001b[0;32m     10\u001b[0m filenames \u001b[38;5;241m=\u001b[39m test_generator\u001b[38;5;241m.\u001b[39mfilenames\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    183\u001b[0m         filepath,\n\u001b[0;32m    184\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    186\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    190\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[1;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(filepath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'D:\\CNN.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = tf.keras.models.load_model(r'D:\\CNN.h5')\n",
    "\n",
    "# Prediction and Confusion Matrix\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(test_generator)\n",
    "y_prob = []\n",
    "y_act = []\n",
    "\n",
    "test_generator.reset()\n",
    "for _ in range(nb_samples):\n",
    "    X_test, y_test = next(test_generator)  # Use next() function here\n",
    "    y_prob.append(model.predict(X_test))\n",
    "    y_act.append(y_test)\n",
    "\n",
    "predicted_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_prob]\n",
    "actual_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_act]\n",
    "\n",
    "# Create a DataFrame for the actual vs predicted classes\n",
    "out_df = pd.DataFrame(np.vstack([predicted_class, actual_class]).T, columns=['predicted_class', 'actual_class'])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = pd.crosstab(out_df['actual_class'], out_df['predicted_class'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "sns.heatmap(confusion_matrix, cmap=\"Blues\", annot=True, fmt='d')\n",
    "plt.show()\n",
    "\n",
    "# Print test accuracy\n",
    "accuracy = np.diag(confusion_matrix).sum() / confusion_matrix.sum().sum() * 100\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee2816-6783-463b-b159-9b3382223887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model.save('CNN_garbage_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72d2fb-4247-4fb6-b451-72df18bda09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "test_dir = r\"D:\\\"\n",
    "\n",
    "# ImageDataGenerator for test set\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Create the test data generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get the number of test samples\n",
    "nb_samples = test_generator.samples\n",
    "\n",
    "# Initialize lists to store predictions and actual labels\n",
    "y_prob = []\n",
    "y_act = []\n",
    "\n",
    "# Reset test generator before making predictions\n",
    "test_generator.reset()\n",
    "\n",
    "# Loop through test data and predict\n",
    "for _ in range(nb_samples // test_generator.batch_size):\n",
    "    X_test, y_test = next(test_generator)  # Get next batch of images\n",
    "    y_prob.append(model.predict(X_test))  # Predict probabilities\n",
    "    y_act.append(y_test)  # Append actual labels\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_prob = np.concatenate(y_prob)\n",
    "y_act = np.concatenate(y_act)\n",
    "\n",
    "# Get predicted and actual class indices\n",
    "predicted_class = np.argmax(y_prob, axis=1)\n",
    "actual_class = np.argmax(y_act, axis=1)\n",
    "\n",
    "# Mapping class indices to class names\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "predicted_class_names = [class_labels[i] for i in predicted_class]\n",
    "actual_class_names = [class_labels[i] for i in actual_class]\n",
    "\n",
    "# Create a DataFrame for the actual vs predicted classes\n",
    "out_df = pd.DataFrame({\n",
    "    'predicted_class': predicted_class_names,\n",
    "    'actual_class': actual_class_names\n",
    "})\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_mtx = pd.crosstab(out_df['actual_class'], out_df['predicted_class'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, cmap=\"Blues\", annot=True, fmt='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print test accuracy\n",
    "accuracy = np.diag(confusion_mtx).sum() / confusion_mtx.sum().sum() * 100\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Generate and print classification report\n",
    "print('Classification Report')\n",
    "print(classification_report(actual_class_names, predicted_class_names, target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cfb56-5755-4153-81cc-661d8b64e1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
